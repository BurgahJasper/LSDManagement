# -*- coding: utf-8 -*-
"""scrapewebdata_snopes.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sXsjh1DJt-vi__ipyG7gQpBjeNGOkL1B
"""

#install packages
!pip3 install requests
!pip3 install bs4

#import packages
import urllib.request
import time
from bs4 import BeautifulSoup
import requests
import pandas as pd

# Reinitialize every time yyou run the program create lists to store data scraped from web
authors = []
dates = []
statements = []
description = []



def scrape_data(page_number):
    page_num = str(page_number)
    print('scraping page number', page_num)
    URL= 'https://www.snopes.com/fact-check/?pagenum='+page_num
    webpage = requests.get(URL)
    soup = BeautifulSoup(webpage.text,"html.parser")

    article_quote = soup.find_all('h3', attrs={'class':'article_title'}) #Get the tag and it's class
    article_author =  soup.find_all('div',attrs={'class':'author_name_box'})  #Get the tag and it's class
    article_date = soup.find_all('span', attrs={'class':'article_date'})#Get the tag and it's class
    article_description = soup.find_all('span', attrs={'class':'article_byline'})#Get the tag and it's class
   
    #loop through heading
    for i in article_quote:
        text = i.text.strip()
        statements.append(text)
    print('statement length',len(statements))
    #Loop through author
    for i in article_author:
        text = i.find_all('span')[0]
        authors.append(text)
    print('authors length',len(authors))
    #Loop through dates
    for i in article_date:
        text = i.text.strip()
        dates.append(text)
    print('dates length',len(dates))        
    #Loop through description
    for i in article_description:
        text = i.text.strip()
        description.append(text)
    print('description length',len(description))

#Loop over 1 to 983 pages
for i in range(1, 983):
  scrape_data(i)

#Create a new dataFrame 

data = pd.DataFrame(columns = ['author',  'statement', 'date', 'description']) 
data['author'] = authors
data['statement'] = statements
data['date'] = dates
data['description'] = description

#show dataset
data.head()

#show dataset
data

#Store the results of scraping first 9 pages to a CSV file
data.to_csv('Snopes Data 1-982.csv')

