# -*- coding: utf-8 -*-
"""scrapewebdata_politifact.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12kEKA0iAmY4eRZTirr_0lH5y4zC-htuA
"""

#install packages
!pip3 install requests
!pip3 install bs4

#import packages
import urllib.request
import time
from bs4 import BeautifulSoup
import requests
import pandas as pd

# Reinitializ every time yyou run the program create lists to store data scraped from web
authors = []
dates = []
statements = []
sources = []
targets = []

def scrape_data(page_number):
    page_num = str(page_number)
    print('scraping page number', page_num)
    URL= 'https://www.politifact.com/factchecks/list/?page='+page_num
    webpage = requests.get(URL)
    soup = BeautifulSoup(webpage.text,"html.parser")
    statement_quote = soup.find_all('div', attrs={'class':'m-statement__quote'}) #Get the tag and it's class
    statement_footer =  soup.find_all('footer',attrs={'class':'m-statement__footer'})  #Get the tag and it's class
    statement_meta = soup.find_all('div', attrs={'class':'m-statement__meta'})#Get the tag and it's class
    target = soup.find_all('div', attrs={'class':'m-statement__meter'}) #Get the tag and it's class
   
    #loop through the footer class m-statement__footer to get the date and author
    for i in statement_footer:
        text = i.text.strip()
        name_and_date = text.split()
        full_name = name_and_date[1]+' '+name_and_date[2]
        date = name_and_date[4]+' '+name_and_date[5]+' '+name_and_date[6]
        dates.append(date)
        authors.append(full_name)

    #Loop through the div m-statement__quote to get the link
    for i in statement_quote:
        stmt_quote_links = i.find_all('a')
        statements.append(stmt_quote_links[0].text.strip())

    #Loop through the div m-statement__meta to get the source
    for i in statement_meta:
        link3 = i.find_all('a') #Source
        source_text = link3[0].text.strip()
        sources.append(source_text)
        
    #Loop through the target or the div m-statement__meter to get the facts about the statement (True or False)
    for i in target:
        fact = i.find('div', attrs={'class':'c-image'}).find('img').get('alt')
        targets.append(fact)

#Loop over 100 to 200 pages
for i in range(100, 151):
  scrape_data(i)

#Create a new dataFrame 

data = pd.DataFrame(columns = ['author',  'statement', 'source', 'date', 'target']) 
data['author'] = authors
data['statement'] = statements
data['source'] = sources
data['date'] = dates
data['target'] = targets

#show dataset
data.head()

#we are only onterested in pants-fire and false news articles to create our fake news dataset
fake = ['pants-fire','false']
fake_df = data[data['target'].isin(fake)]
fake_df.shape

#show dataset
fake_df

#Store the results of scraping first 9 pages to a CSV file
fake_df.to_csv('fake_news100-150.csv')

